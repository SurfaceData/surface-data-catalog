# HuggingFace Datasets

This subdirectory provides HuggingFace dataset libraries.

We manage this very carefully to automatically manage the datasets based on
what's stored in the database.  We list some key steps below during development
time.

## Usage

Using the datasets managed on HuggingFace require the following pattern:

```py
from datasets import load_dataset
load_dataset('SurfaceData/Catalog',
             lang_pair, # ex: 'en_ig'
             use_auth_token=your_hugging_face_auth_token,
             apikey=your_surface_catalog_api_key)
```

While the repos are set as private HuggingFace hub requires `use_auth_token`.
That can either be `true` if logged in via the command line tool or it can be
an API key with read access.

In all cases, we require the custom dataset arg `apikey` which is the apikey
generated by the data catalog.  This will be forwarded to the dataset download
request and be used for authentication.

## Management Steps

1.  Create the new dataset hub on HuggingFace
1.  Add the dataset hub as a remote: `git remote add -f dataset-newhubname https://path/to/hub`
1.  Create the subdirectory as a subtree: `git subtree add --prefix datasets/newhubname dataset-newhubname main`
1.  Make changes as needed and commit
1.  Push the subtree to HuggingFace: `git subtree push --prefix datasets/newhubname dataset-newhubname main`

We followed the [Atlassian Git
Subtree](https://www.atlassian.com/git/tutorials/git-subtree) docs to setup
these steps.  By following these steps we can manage each subdirectory as a hub
dataset and update the hub as needed.

Soon, we'll prepare a template that'll live in this repo which can be used as
the basis for every dataset generated and then setup some automation to
auto-create the repo, populate it, and then simplify pushing.
